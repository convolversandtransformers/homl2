{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Ensemble Learning and Random Forests\n",
    "\n",
    "**Keywords:** Bagging, Boosting, Stacking\n",
    "\n",
    "## Voting Classifier\n",
    "\n",
    "### Hard Voting Classifier: \n",
    "\n",
    "**Keywords:** Weak & Strong Learners\n",
    "\n",
    "- Enough weak learners with sufficient diversity can be stacked together, the voting classifier becomes a strong learner.\n",
    "- Works on the **law of large numbers**; more predictors more steadier outcome.\n",
    "- However, each weak learner should learn **independent** priors for this to happen.\n",
    "- It's impossible since all predictors are trained on the same data.\n",
    "- How to achieve different priors on same data? Try different algo.\n",
    "\n",
    "### Soft Voting Classifier:\n",
    "\n",
    "Average predicted logits/probabilities across all models. This way highly confident classes will be weighted higher.\n",
    "\n",
    "## Bagging and Pasting\n",
    "\n",
    "**Keywords:** bagging, pasting, bootstrap, out-of-bag, random patches and random sub-spaces\n",
    "\n",
    "- Bagging: sampling w/ replacement, also called **bootstrap aggregating**\n",
    "\n",
    "- Pasting: sampling w/o replacement\n",
    "\n",
    "- Bagging **better** than pasting, since aggregating leads to lower variance and bias.\n",
    "\n",
    "- Bagging Classifier (DT) vs Random Forest Classifier: API + Optimization differences\n",
    "\n",
    "- ```bash\n",
    "  # Self Note:\n",
    "  The trade-off between increasing the performance on dataset by 1-5% vs trying to build a continuously learning pipeline.\n",
    "  \n",
    "  Viz is important, but not always possible on high-dimensional dataset.\n",
    "  ```\n",
    "\n",
    "### Out-of-bag Evaluation\n",
    "\n",
    "During evaluation using bagging approach, only 63% (only for a sufficiently high number of instances) on average is seen during training by each predictor. Hence an `oob` instances can function as validation set for validating model performance.\n",
    "\n",
    "```bash\n",
    "# Self Note:\n",
    "In absence of enough samples (when training a bagging model), oob score can be used as a substitute for validation/hold-out set performance.\t\n",
    "```\n",
    "\n",
    "### Random Patches and Random Subspaces\n",
    "\n",
    "- Random Patches: Sampling input samples and feature\n",
    "- Random Subspaces: Taking all training instances but bootstrapping the features (ex: RF)\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "- Ensemble of DT\n",
    "- Bagging and Pasting\n",
    "\n",
    "| Random Forest                                | Bagging Classifier                                |\n",
    "| -------------------------------------------- | ------------------------------------------------- |\n",
    "| Trained on all samples                       | Subsampling can be done                           |\n",
    "| Bootstrapping features is handled by default | Bootstrapping features and samples can be handled |\n",
    "\n",
    "### Extra Trees\n",
    "\n",
    "| Random Forest                                       | Extra Trees                                                  |\n",
    "| --------------------------------------------------- | ------------------------------------------------------------ |\n",
    "| A random subset of features considered at each node | Have random thresholds set for each feature                  |\n",
    "| DTs optimize the best threshold for each split      | Faster than RF because it doesn't have to calculate threshold values |\n",
    "\n",
    "- Is Extra Trees better than Random Forest? Answer: No free lunch. \n",
    "\n",
    "### Feature Importance\n",
    "\n",
    "- A weighted average across the nodes in all trees, computed by looking at how much impurity was reduced by the tree nodes using that particular feature.\n",
    "- Nodes weight is calculated by how many features are associated with it.\n",
    "\n",
    "## Boosting\n",
    "\n",
    "- Combines weak learners sequentially to create strong learners\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "- Sequentially train predictors, iteratively improving on the mis-classified instances of the previous predictor by updating the instance weights and training the next predictor with the weighted instances.\n",
    "- Hyper-parameter: Learning Rate: 1 vs 0.5 (Adaptive or not **?**)\n",
    "- The update procedure is similar to gradient descent but the update doesn't back propagate, instead new learners are added to make the ensemble predictions better.\n",
    "- **Not Parallelizable**\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "```\n",
    "TODO: Add math here\n",
    "```\n",
    "\n",
    "- sklearn uses **SAMME**.\n",
    "- For binary predictions AdaBoost is the same as **SAMME**. \n",
    "- For softer, probabilities based prediction sklearn also has **SAMME.R** algorithm which works on prediction probabilities.\n",
    "\n",
    "\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "Keywords: GBRT (Gradient Boosting Regression Trees)\n",
    "\n",
    "- Doesn't tweak instance weights like AdaBoost.\n",
    "- Fits the new predictor on the residual learning error of the previous predictor.\n",
    "- Prediction is the **sum** of prediction by all models.\n",
    "- Hyper-paramter tweaking:\n",
    "  - LR is low; increase the no. of estimators.\n",
    "  - LR is high; decrease LR and/or decrease the no. of estimators\n",
    "  - Add some other regularization techniques\n",
    "- sklearn API: **staged_predict**\n",
    "- You can use early-stopping (w/ **warm_start = True**)\n",
    "- Subsampling here is referred to as Stochastic Gradient Boosting and as usual reduces variance at the cost of increasing bias.\n",
    "- Preferred library: XGBoost\n",
    "\n",
    "## Stacking\n",
    "\n",
    "- Instead of using averaging or some other algorithm to decide the output of the ensemble of predictors, why not use a predictor for averaging (weighting the predictions) itself?\n",
    "\n",
    "- The idea behind stacking is to split the data into subsets equal to the no. of layers in the stack.\n",
    "\n",
    "- For a 2-layer stack (one predictor layer and one blender layer on top); the data is split into two subsets, A and B.\n",
    "\n",
    "  - The initial layer (ensemble of predictors) is trained on the subset_A. After the training, the subset_B which acts as a hold-out set is used and predictions are generated using the initial layer.\n",
    "\n",
    "  - The predictions with the respective ground-truths combined forms the training data for the blender layer.\n",
    "\n",
    "  - Finally after optimizing the layers on separate data subsets, the models/layers of models are stacked together to form a stacked ensemble.\n",
    "\n",
    "  - This logic is extendable to multiple layers. However, if you want to create a stack of 20 or so layers/models put together with enough data to train, then why not just use an optimized neural network instead of all this unnecessary work. Be logical :P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
